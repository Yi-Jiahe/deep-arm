# Deep Arm
Deep arm is an AI controlled robotic arm trained using inverse kinematic data in TensorFlow. This is a simple THREE.js webapp that allows you to play with the trained model. 

## Training
The arm is a 3-link robotic arm, with link 0 being the fixed link at the ground. Each joint has 2-degrees of freedom, being able to rotate in two axes.

The model input parameters are the desired coordinates for the manipulator of the arm, and the output parameters are joint angles for each of the joints. The training data was generated by generating a matrix of points in a rectangular cuboid centered at the base of the arm, above ground level. The loss function used was a combination of the error as well as a comfort function. The error was calcuated using forward kinematics of the joint angles to find the position of the manipulator, and the euclidean distance between its actual position and desired position was taken as the error. The comfort function penalized the model if joint angles were chosen outside the actuation limits of the arm.

## QnA
Q: Questions about the design of the arm.

A: The focus of this project was mainly to explore what it was like to train a model. A robotic arm was chosen as the concept as I am interested in robotics and it allowed me an opportunity to generate an arbitary amount of data depending on how fine the divison between points were. Not much thought went into the design of the arm; a 3-link model was chosen to keep it simple and the DoF of each joint were chosen to allow it to reach any space within a two-link radius within the actuation limits of the joints, i.e. a hemisphere above ground level.

In hindsight there are things I would like to improve on regarding the design of the robotic arm. Firstly, the second joint need not have 2-DoF, 1 would have theoretically been sufficient, evidenced by the small actuation angles for the DoF (also in part due to the loss function choosen). A more realistic model would probably also have been a plus.


Q: Questions about the training data used.

A: In hindsight, again, there are two main things I would like to improve on. Firstly, the cuboid space within which the model was trained resulted in a larger amount of training data for the corners of the space, despite the uniform nature of yaw angle for the robot. This could have introduced some unwanted bias to the arm related to the corners, and could have been avoided if the points were sampled in a hemisphere around the base of the arm. This issue arose due to an oversight when the generation algorithm was redesigned. Originally the points were generated with a radial coordinate system to better match the reach of the arm, but again there was a problem of a higher concentration of points towards the pole of the hemisphere. The cuboid was chosen to ensure the points were evenly distributed through space. To address this issue, the points could have been generated using a cartesian coordinate system, and then filtered based on their distance to the base of the arm, forming a hemisphere of evenly distributed points. Secondly, the points were not shuffled which the algorithm could have picked up on, learning something to do with the order the training data was presented in. A simple fix would be to shuffle the training set.


Q: I noticed that there is a dead-zone where the model struggles to reach the desired point.

A: My guess is that this due to the way the comfort loss interacts with the error. Another manifestation of this problem is when the target is far outside the reach of the arm. Likely due to the high error, the model chooses some position to minimise its comfort loss instead. I would like to improve the loss algorithms if I ever return to this project as it is not a trivial problem to solve.
